#!/bin/bash
#
# Main script to run report generation batch job

set -eu

start_time=$(date +%s)

RUNID="$1"
LOG_FILE="$2"
REPORT_RUN_DATE="$3"
# REPORT_FILE_NAME="$4"

APP_HOME=$( cd "$(dirname "$0")/.." && pwd )
source $APP_HOME/scripts/common-utils.sh

exec >> $LOG_FILE 2>&1
log "Job($RUNID) starts with configuration file '$CONFIG_FILE'"

# Clean up the log files generated by SAS
for file in $APP_HOME/nohup.*; do
  if [ -e $file ]; then
    rm $file
  fi
done

year=`date -d"$REPORT_RUN_DATE -7 months" +%Y`
quarter=$(( ($(date -d"$REPORT_RUN_DATE -7 months" +%-m)-1)/3+1 ))
year_quarter="${year}_${quarter}"

rep_start=$(date -d"$REPORT_RUN_DATE -7 months" +01%b%Y)
rep_start=`echo "$rep_start" | tr '[:lower:]' '[:upper:]'`
rep_end=$(date -d"$rep_start +1 month -1 day" +%d%b%Y)
rep_end=`echo "$rep_end" | tr '[:lower:]' '[:upper:]'`

case "$quarter" in
  1)
  date_start="$year/01/01"
  date_end=`date -d"$year/04/01 -1 day" +%Y/%m/%d`
  ;;
  2)
  date_start="$year/04/01"
  date_end=`date -d"$year/07/01 -1 day" +%Y/%m/%d`
  ;;
  3)
  date_start="$year/07/01"
  date_end=`date -d"$year/10/01 -1 day" +%Y/%m/%d`
  ;;
  4)
  date_start="$year/10/01"
  date_end=`date -d"$year/01/01 +1 year -1 day" +%Y/%m/%d`
  ;;
esac

emp_id_name_mapping_csv=$INPUT_DATA_PATH/emp_id_name_mapping/emp_id_name_mapping.csv
emp_id_dept_mapping_csv=$INPUT_DATA_PATH/emp_id_dept_mapping/emp_id_dept_${year}Q${quarter}.csv
emp_by_dept=$emp_by_dept/emp_by_dept.csv
emp_by_id=$emp_by_id/emp_by_id.csv

log "Input parameters are RUNID:$RUNID, year:$year, quarter:$quarter, date_start=$date_start, date_end=$date_end, rep_start:$rep_start, rep_end:$rep_end"

log "Clean up existing output files"
if [ -f "$emp_by_dept" ]; then
  rm $emp_by_dept
fi

log "Extracting Hive table ..."
dept_year=$year
dept_start_month=`date -d"$date_start" +%Y%m`
dept_end_month=`date -d"$date_end" +%Y%m`
quarter_name=${year}Q${quarter}
echo ssh ${REMOTE_USER_ID}@${REMOTE_HOST_IP} "sh ${REMOTE_REPORT_HOME}/bin/extract_emp_data.sh $RUNID $dept_year $dept_start_month $dept_end_month $quarter_name"

emp_id_dept_mapping_csv_zip=${emp_id_dept_mapping_csv}.zip
log "Copying extracted depts from REMOTE and write into '$emp_id_dept_mapping_csv_zip' ..."
sftp ${REMOTE_USER_ID}@${REMOTE_HOST_IP}:${REMOTE_REPORT_OUTPUT_DATA_PATH}/emp_id_dept_mapping/emp_id_dept_mapping_${quarter_name}.csv.zip $emp_id_dept_mapping_csv_zip

log "Extracting downloaded file '$emp_id_dept_mapping_csv_zip' and write into '$emp_id_dept_mapping_csv'"
unzip -op $emp_id_dept_mapping_csv_zip > $emp_id_dept_mapping_csv

log "Remove downloaded file '$emp_id_dept_mapping_csv_zip'"
rm $emp_id_dept_mapping_csv_zip

# Proceed only if both the input files are present
if [ -f $emp_id_dept_mapping_csv -a -f $emp_id_name_mapping_csv ]; then
  log "Processing ..."
  hive -f job.hql # ||:
  JOB_STATUS=$?
  log "HIVE processing completed with status '$JOB_STATUS'"
else
  log "HIVE processing skipped. REASON: One or more input file(s) doesn't exists!."
  log "Make sure files '$emp_id_dept_mapping_csv' and '$emp_id_name_mapping_csv' are exists."
  exit 1
fi

# Delete files older than N days
log "Deleting older log files"
find $LOG_PATH -mtime +15 -type f -delete

end_time=$(date +%s)
log "Job(${RUNID}) took $[($end_time - $start_time)/60] minutes to complete"
